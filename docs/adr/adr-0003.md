# ADR 0003: Adoption of jan-v1-4v for iOS LLM RAG App

## Status
Accepted

## Context
The `jan-v1-4v` model, a lightweight vision-capable large language model (LLM) in GGUF format, has recently been released. This model enables efficient on-device inference on iOS devices, opening new opportunities for offline retrieval-augmented generation (RAG) and multimodal input processing without relying on large cloud-based models. The availability of `jan-v1-4v` supports enhanced user experiences by reducing latency and enabling offline functionality.

## Decision
We have decided to integrate `jan-v1-4v` as the default on-device model for the iOS LLM RAG app. This model will replace or supplement prior models to leverage its lightweight and vision-capable features. Existing models will remain accessible as optional configurations to maintain flexibility.

The model pipeline will be updated to support both GGUF format downloads and Colab-based conversion workflows, making GGUF an optional path rather than a mandatory one. This approach ensures compatibility and ease of use for various deployment scenarios.

## Consequences
### Benefits
- Lower latency due to on-device inference
- Offline support enabling use without internet connectivity
- Reduced dependency on cloud infrastructure, improving privacy and cost-efficiency
- Support for multimodal input through the vision capabilities of `jan-v1-4v`

### Trade-offs
- Potentially lower accuracy or knowledge breadth compared to larger cloud-hosted models
- Need to update the app architecture to accommodate the new model format and inference pipeline
- Additional testing and validation required to ensure performance and stability on iOS devices

Overall, adopting `jan-v1-4v` advances our goal of delivering a responsive, privacy-conscious, and versatile LLM RAG experience on iOS.
