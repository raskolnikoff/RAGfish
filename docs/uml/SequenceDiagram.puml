@startuml
!theme amiga
actor User

User -> Settings : load()
alt Embedded resources available
    Settings --> DocumentManager : provide embedded RAGpack (.zip), LLMModel
else Embedded resources NOT available
    User -> DocumentManager : selectImportRAGpack()

    alt RAGpack (.zip) available
        alt Source: Local File
            DocumentManager -> RAGpack : extractAndParse(RAGpack.zip)
            DocumentManager --> User : importComplete()
        else Source: Cloud (optional)
            DocumentManager -> GoogleDriveService : download("file.zip")
            GoogleDriveService --> DocumentManager : RAGpack (.zip)
            DocumentManager -> RAGpack : extractAndParse(RAGpack.zip)
            DocumentManager --> User : importComplete()
        end
    else RAGpack (.zip) NOT found
        DocumentManager --> User : error("RAGpack (.zip) not found")
        note right: Fallback to embedded resources if available; otherwise, user must import manually
    end
end

' Ensure LLM model is ready (default text model, optional vision model)
User -> Settings : getDefaultModel()
Settings --> User : defaultLLM
User -> LLMModel : ensureLoaded(defaultLLM)
alt Model not loaded
    User -> LLMModel : loadModel(file)
    LLMModel --> User : modelReady
else Model already loaded
    LLMModel --> User : modelReady
end
note right of LLMModel
  Default on-device text model may be Jan‑v1‑4B (GGUF).
  Vision model (jan‑v1‑4v) is optional and only used when enabled.
end note

User -> UserQuery : ask(question[, image])
UserQuery -> Settings : getPromptTemplate()
Settings --> UserQuery : promptTemplate
alt Question includes image
    UserQuery -> LLMModel : embedImage(image)
    LLMModel --> UserQuery : imageQueryEmbedding
    UserQuery -> VectorStore : findRelevant(imageQueryEmbedding, modality=vision)
    VectorStore --> UserQuery : relevantImageChunks
    UserQuery -> LLMModel : embed(question)
    LLMModel --> UserQuery : queryEmbedding
    UserQuery -> VectorStore : findRelevant(queryEmbedding, modality=text)
    VectorStore --> UserQuery : relevantTextChunks
else Text-only question
    UserQuery -> LLMModel : embed(question)
    LLMModel --> UserQuery : queryEmbedding
    UserQuery -> VectorStore : findRelevant(queryEmbedding, modality=text)
    VectorStore --> UserQuery : relevantChunks
end

UserQuery -> AnswerGenerator : generate(merge(relevantTextChunks, relevantImageChunks), question, promptTemplate)
AnswerGenerator --> UserQuery : answer

UserQuery --> User : displayAnswer(answer)
note right
  QA thread/history management (multi question-answer pairs) is implemented in NoesisNoema.
  Multimodal entries (image + text) are stored alongside text-only entries when vision is enabled.
end note
@enduml